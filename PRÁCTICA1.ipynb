{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg4ewsRSmGOw",
        "outputId": "24a0e285-37b1-4e61-9fb9-a1f3c1add6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping las 250 mejores películas de IMBb\n",
            "User-Agent utilizado: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\n"
          ]
        }
      ],
      "source": [
        "# Comenzamos importando las librerias necesarias\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "# Comenzamos creando una función inicializadora que nos ayudará a identificar la\n",
        "# página escecífica donde queremos hacer el scrapping.\n",
        "class IMDbScraper:\n",
        "    def __init__(self):\n",
        "        self.url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
        "        self.data = []\n",
        "        self.headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"\n",
        "        }\n",
        "# La siguiente función será específica para verificar el hearder y el user-agent\n",
        "    def check_user_agent(self):\n",
        "        print(f\"User-Agent utilizado: {self.headers['User-Agent']}\")\n",
        "\n",
        "# Continuaremos creando otra funciòn que nos permita descargar la web objetivo en html\n",
        "    def download_html(self):\n",
        "        response = requests.get(self.url, headers=self.headers)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            print(f\"Error al descargar la página. Estado: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "# Ahora configuramos función donde podamos obtener los datos básicos de las 250 películas\n",
        "    def scrape_data(self, html):\n",
        "        start = html.find('<script type=\"application/ld+json\">')\n",
        "        end = html.find('</script>', start)\n",
        "        json_data = html[start + len('<script type=\"application/ld+json\">'):end].strip()\n",
        "\n",
        "        data = json.loads(json_data)\n",
        "# Para ejecutar esta función, hacemos una iteración para cada uno de los datos que queremos obtener.\n",
        "        for item in data['itemListElement']:\n",
        "            movie = item['item']\n",
        "            title = movie['name']\n",
        "            rating = movie['aggregateRating']['ratingValue']\n",
        "            genre = movie.get('genre', 'N/A')\n",
        "            web = movie['url']\n",
        "            self.data.append([title, rating, genre, web])\n",
        "\n",
        "# Para los datos más complejos, creamos una función a parte con BeautifulSoup.\n",
        "    def scrape_movie_details(self, url):\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            json_data = soup.find(\"script\", type=\"application/ld+json\").string\n",
        "            movie_data = json.loads(json_data)\n",
        "\n",
        "            date = movie_data.get(\"datePublished\", \"N/A\")\n",
        "            director = \", \".join([d[\"name\"] for d in movie_data.get(\"director\", [])])\n",
        "            writers = \", \".join([w[\"name\"] for w in movie_data.get(\"creator\", []) if w[\"@type\"] == \"Person\"])\n",
        "            actors = \", \".join([a[\"name\"] for a in movie_data.get(\"actor\", [])])\n",
        "\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "\n",
        "            return date, director, writers, actors\n",
        "        else:\n",
        "            print(f\"Error al acceder a la URL: {url}\")\n",
        "            return \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "# Ahora tenemos una función que genera el dataframe con toda la información obtenida previamente.\n",
        "    def save_to_dataframe(self):\n",
        "        df = pd.DataFrame(self.data, columns=[\"Title\", \"Rating\", \"Genre\", \"Web\"])\n",
        "        df[['Date', 'Director', 'Writers', 'Actors']] = df['Web'].apply(lambda url: pd.Series(self.scrape_movie_details(url)))\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# Por último,creamos una función que ejecuta toddo el código\n",
        "    def run(self):\n",
        "        print(\"Scraping las 250 mejores películas de IMBb\")\n",
        "\n",
        "        self.check_user_agent()\n",
        "\n",
        "        html = self.download_html()\n",
        "        if html:\n",
        "            self.scrape_data(html)\n",
        "            df = self.save_to_dataframe()\n",
        "            display(df)\n",
        "            df.to_csv('films.csv', index=False)\n",
        "        else:\n",
        "            print(\"No se pudo obtener HTML.\")\n",
        "\n",
        "scraper = IMDbScraper()\n",
        "scraper.run()\n"
      ]
    }
  ]
}